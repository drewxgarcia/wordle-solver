Optimize Character to Bit Mapping:

Currently, you use arrays CHAR_TO_BIT_MAP and BIT_TO_CHAR_MAP for character to bit and vice versa conversions. These are fine for small datasets, but they can be optimized further. Consider using bitwise operations for these conversions, which can be more efficient than array indexing.
Efficient String Handling in int_to_word and simulate_results:

In int_to_word, instead of pushing chars to a vector and then collecting into a string, you could use a fixed-size array and fill it directly, avoiding the need for a dynamic vector and the associated memory allocations.
In simulate_results, you might optimize string concatenation by pre-allocating the string with the expected capacity (String::with_capacity) to avoid reallocations.
Parallel Processing in make_guess:

You're using parallel processing with rayon, which is a good choice for intensive computations. However, consider the overhead of cloning self.word_list into an Arc. If self.word_list is large, this could be a significant overhead. Investigate if there's a way to share the word list among threads without cloning, such as using shared references.
Refactoring process_results:

The current approach of collecting words to keep and then replacing self.word_list is correct for avoiding mutable and immutable borrow issues. However, this might be optimized further. If the number of words to remove is small compared to the total, it might be more efficient to remove them directly from self.word_list rather than creating a new collection and reassigning it.
Using More Specific Data Structures:

Examine if HashSet and HashMap are the most efficient structures for your use cases. For example, if the order of elements in known_wrong_positions or known_absent is not important and you frequently check for membership, a HashSet is suitable. However, if you need ordered data or are frequently iterating, other structures like Vec or BTreeSet might be more efficient.
Reducing Memory Allocations:

Look for opportunities to reduce memory allocations. For instance, if certain data structures are repeatedly allocated and deallocated, consider reusing them across function calls.
Consider Laziness in calculate_entropy:

The calculate_entropy function might benefit from lazy evaluation where possible. For instance, the pattern_counts HashMap can potentially grow large, and optimizing its construction and iteration could yield performance benefits.
Error Handling:

In load_word_list, the expect statement is used, which will cause the program to panic if the file can't be opened. Consider more robust error handling that doesn't result in a panic, such as returning a Result type and handling the error in the main function.